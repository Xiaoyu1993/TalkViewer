{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 5816\n",
      "Total nodes: 820\n",
      "Total links: 557\n",
      "Total speakers: 10\n"
     ]
    }
   ],
   "source": [
    "###### import csv\n",
    "import re\n",
    "import spacy\n",
    "import json\n",
    "from spacy import displacy\n",
    "from textblob import TextBlob\n",
    "from queue import *\n",
    "\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# pre-processing\n",
    "def PreProcess(senSet):\n",
    "    #remove content between [ ]\n",
    "    print(\"Pre-processing...\")\n",
    "    for index in range(len(senSet)):\n",
    "        while senSet[index].find('[')>=0:\n",
    "            i_start = senSet[index].find('[')\n",
    "            i_end = senSet[index].find(']')\n",
    "            s = senSet[index][i_start:i_end+2]\n",
    "            senSet[index] = senSet[index].replace(s, \"\")\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, text='', frequency=1, categ='', speaker='', start_word=0, end_word=0):\n",
    "        self.text = text\n",
    "        self.frequency = frequency\n",
    "        self.categ = categ # category of the named entity (10 categories)\n",
    "        self.speaker = speaker\n",
    "        self.start_word = start_word # word index start from 0\n",
    "        self.end_word = end_word\n",
    "        \n",
    "    def Print(self):\n",
    "        print (self.name + ' ' + str(self.frequency) + ' ' + self.categ)\n",
    "\n",
    "class Link:\n",
    "    def __init__(self, Id=-1, source='', target='', sourceCateg='', targetCateg='', distance=-1.0, speaker='', frequency=1):\n",
    "        self.id = Id # the id of the link is only decided by source and target, not speaker\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.sourceCateg = sourceCateg\n",
    "        self.targetCateg = targetCateg\n",
    "        self.distance = distance # the average of all distances\n",
    "        self.speaker = speaker\n",
    "        self.frequency = frequency\n",
    "        \n",
    "    def Print(self):\n",
    "        print(self.source + ' ' + self.target + ' ' + str(self.distance) + ' ' + str(self.frequency))\n",
    "\n",
    "class Mark:\n",
    "    def __init__(self, text='', categ='', start_char=0, end_char=0):\n",
    "        self.text = text\n",
    "        self.categ = categ # category of the named entity (10 categories)\n",
    "        self.start_char = start_char\n",
    "        self.end_char = end_char\n",
    "        \n",
    "class MarkedSen:\n",
    "    def __init__(self, id, text='', speaker='', marks=[]):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.speaker = speaker\n",
    "        self.marks = marks\n",
    "\n",
    "'''class Speaker:\n",
    "    def __init__(self, id=-1, name='', utterList=[]):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.utterList = utterList\n",
    "        \n",
    "    def Print(self):\n",
    "        print(str(self.id) + ' ' + self.name)\n",
    "        print(self.utterance)'''\n",
    "\n",
    "class tempSpeaker:\n",
    "    def __init__(self, name='', utterSet={}):\n",
    "        self.name = name\n",
    "        self.utterSet = utterSet\n",
    "\n",
    "class tempConv:\n",
    "    def __init__(self, source='', target='', utterSet={}):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.utterSet = utterSet\n",
    "        \n",
    "def FormatLable(label):\n",
    "    if label == 'PERSON':\n",
    "        return 'Person'\n",
    "    elif label == 'QUANTITY':\n",
    "        return 'Measure'\n",
    "    elif label == 'FAC' or label == 'GPE' or label == 'LOC':\n",
    "        return 'Geo'\n",
    "    elif label == 'MONEY':\n",
    "        return 'Unit'\n",
    "    elif label == 'DATE' or label == 'TIME':\n",
    "        return 'Time'\n",
    "    elif label == 'NORP'or label == 'ORG':\n",
    "        return 'Organization'\n",
    "    else:\n",
    "        return None   \n",
    "    \n",
    "# extract one triple from given sentence\n",
    "def ExtractNER(line, index):\n",
    "    #print(line['text'])\n",
    "    localList = []\n",
    "    \n",
    "    # parse sentence\n",
    "    doc = nlp(str(line['text']))\n",
    "\n",
    "    # node related\n",
    "    # basic categories\n",
    "    for e in doc.ents:\n",
    "        #ents = [(e.text, e.start_char, e.end_char, e.label_) ]\n",
    "        #print(ents)\n",
    "        \n",
    "        categ = FormatLable(e.label_)\n",
    "        if categ != None:\n",
    "            # for node-link diagram\n",
    "            if e.text in nodeSet:\n",
    "                nodeSet[e.text].frequency += 1\n",
    "                localList.append(nodeSet[e.text])# add current token to local set\n",
    "            else:\n",
    "                m_node = Node(e.text, 1, categ, line['speaker'], line['text'][:e.start_char].count(' '), line['text'][:e.end_char].count(' '))\n",
    "                nodeSet[e.text] = m_node\n",
    "                localList.append(nodeSet[e.text])# add current token to local set\n",
    "\n",
    "            # for highlighted sentence\n",
    "            m_mark = Mark(e.text, categ, e.start_char, e.end_char)\n",
    "            lineList[index]['marks'].append(m_mark.__dict__)\n",
    "            \n",
    "    # emotion indicators\n",
    "    for token in doc:\n",
    "        #print(token.sentiment)\n",
    "        testimonial = TextBlob(token.text)\n",
    "        if testimonial.sentiment.polarity > 0.5 or testimonial.sentiment.polarity < -0.5:\n",
    "            #for node-link diagram\n",
    "            if token.text in nodeSet:\n",
    "                nodeSet[token.text].frequency += 1\n",
    "            else:\n",
    "                if testimonial.sentiment.polarity > 0.5:\n",
    "                    m_node = Node(token.text, 1, 'Positive', line['speaker'], token.i, token.i)\n",
    "                else:\n",
    "                    m_node = Node(token.text, 1, 'Negative', line['speaker'], token.i, token.i)\n",
    "                nodeSet[token.text] = m_node\n",
    "            localList.append(nodeSet[token.text])\n",
    "            \n",
    "            # for highlighted sentence\n",
    "            if testimonial.sentiment.polarity > 0.5:\n",
    "                m_mark = Mark(token.text, 'Positive', token.idx, token.idx + len(token.text))\n",
    "            else:\n",
    "                m_mark = Mark(token.text, 'Negative', token.idx, token.idx + len(token.text))\n",
    "            lineList[index]['marks'].append(m_mark.__dict__)\n",
    "    \n",
    "    lineList[index]['marks'].sort(key=lambda x:x['start_char'])\n",
    "    \n",
    "    # link related\n",
    "    # create links by traverse localList\n",
    "    for token1 in localList:\n",
    "        for token2 in localList:\n",
    "            dist = float(token2.start_word-token1.start_word)\n",
    "            if dist >0 and dist <= maxDist and token1.text != token2.text:\n",
    "                #linkList.append(link.__dict__)\n",
    "                link = Link(len(linkSet), token1.text, token2.text, token1.categ, token2.categ, dist, line['speaker'], 1)\n",
    "                # create key by combining the source and target of link alphabetally\n",
    "                key=''\n",
    "                if token1.text <= token2.text:\n",
    "                    key = token1.text + '-' + token2.text\n",
    "                else:\n",
    "                    key = token2.text + '-' + token1.text\n",
    "                    \n",
    "                # add link to linkSet\n",
    "                if key in linkSet:\n",
    "                    linkSet[key]['distance'] = float(linkSet[key]['distance'] * linkSet[key]['frequency'] + dist)/float(linkSet[key]['frequency']+1)\n",
    "                    linkSet[key]['frequency'] += 1\n",
    "                    link.id = linkSet[key]['id'] # this link appears before, update the id to the old one\n",
    "                else:\n",
    "                    linkSet[key] = link.__dict__\n",
    "             \n",
    "                # add link to speakerSet\n",
    "                if line['speaker'] in speakerSet:\n",
    "                    if key in speakerSet[line['speaker']].utterSet:# this person say it before\n",
    "                        curLink = speakerSet[line['speaker']].utterSet[key]\n",
    "                        speakerSet[line['speaker']].utterSet[key]['distance'] = float(curLink['distance'] * curLink['frequency']+dist)/float(curLink['frequency']+1)\n",
    "                        speakerSet[line['speaker']].utterSet[key]['frequency'] += 1\n",
    "                    else:# this person doesn't say it, but someone else say it before\n",
    "                        speakerSet[line['speaker']].utterSet[key] = link.__dict__\n",
    "                else: # no one say it before\n",
    "                    speaker = tempSpeaker(line['speaker'], {})\n",
    "                    speaker.utterSet[key] = link.__dict__\n",
    "                    speakerSet[line['speaker']] = speaker\n",
    "                \n",
    "    # create links with slide window\n",
    "    '''for token in doc:# go over every word in sentence\n",
    "        j = len(slideWin)-1\n",
    "        while j >= 0:# compare current word with the ones in the slide window\n",
    "            if slideWin[j] in localList and token.text in localList:# need to fix to support phrase\n",
    "                link = Link(slideWin[j], token.text, maxDist-j, 1)\n",
    "                linkList.append(link.__dict__)\n",
    "            j = j-1\n",
    "        # update the slide window\n",
    "        if i < maxDist:\n",
    "            slideWin.append(token.text)\n",
    "            i += 1\n",
    "        else:\n",
    "            #print(slideWin.get() + ' ')\n",
    "            print(slideWin.pop(0))\n",
    "            slideWin.append(token.text)\n",
    "        #print(slideWin)'''\n",
    "            \n",
    "    # token level\n",
    "    # ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\n",
    "    # ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\n",
    "    # print(ent_san)  # [u'San', u'B', u'GPE']\n",
    "    # print(ent_francisco)  # [u'Francisco', u'I', u'GPE']\n",
    "\n",
    "    # displacy.serve(doc, style='ent')\n",
    "    \n",
    "    #for token in doc:\n",
    "    #    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "    #          token.shape_, token.is_alpha, token.is_stop)\n",
    "\n",
    "\n",
    "\n",
    "# load Spacy NLP dictionary\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class Line:\n",
    "    def __init__(self, id=0, speaker='', text='', date='', marks=[]):\n",
    "        self.id = id\n",
    "        self.speaker = speaker\n",
    "        self.text = text\n",
    "        self.date = date\n",
    "        self.marks = marks\n",
    "\n",
    "# load data\n",
    "file = open(\"debate.csv\", \"r\", encoding = \"ISO-8859-1\")\n",
    "reader = csv.reader(file)\n",
    "lineList = []\n",
    "utterDict = {} # dictionary for utters of each speaker\n",
    "counter = 0\n",
    "for item in reader:\n",
    "    if counter == 0:\n",
    "        counter += 1\n",
    "        continue\n",
    "    else:\n",
    "        counter += 1\n",
    "    #format sentences in item as string\n",
    "    #fullP = \"\".join(item)\n",
    "    #splitP = fullP.split(\";\", 3);\n",
    "    #splitS = splitP[3][1:len(splitP[3])].split(\".\");\n",
    "    #print(splitS)\n",
    "    #print(item[2])\n",
    "    splitS = re.split(r'(\\. |\\! |\\? |\\.\\.\\.|\\.\\\" )',item[2]) #item[2].split('. ')\n",
    "    splitS.append(\"\")\n",
    "    splitS = [\"\".join(i) for i in zip(splitS[0::2],splitS[1::2])]\n",
    "    for sen in splitS:\n",
    "        line = Line(len(lineList), item[1], sen, item[3], [])\n",
    "        lineList.append(line.__dict__)\n",
    "        #if item[1] in utterDict:\n",
    "        #    utterDict[item[1]] += sen + ' '\n",
    "        #else:\n",
    "        #    utterDict[item[1]] = sen + ' '\n",
    "\n",
    "file.close()\n",
    "print(\"Total sentences: \" + str(len(lineList)))\n",
    "\n",
    "nodeSet = {}\n",
    "nodeList = []# for json store\n",
    "linkSet = {}\n",
    "linkList = []\n",
    "speakerSet = {}\n",
    "speakerList = []\n",
    "convSet = {}\n",
    "convList = []\n",
    "\n",
    "maxDist = 5\n",
    "maxL = 0\n",
    "maxI = 0\n",
    "\n",
    "for index in range(len(lineList)):\n",
    "#index = 3\n",
    "    maxI = ExtractNER(lineList[index], index)\n",
    "\n",
    "# store extracted information to json\n",
    "for key in nodeSet:\n",
    "    node = nodeSet[key].__dict__\n",
    "    nodeList.append(node)\n",
    "    \n",
    "for key in linkSet:\n",
    "    linkList.append(linkSet[key])\n",
    "    \n",
    "outputDict = {\n",
    "    \"nodes\": nodeList,\n",
    "    \"links\": linkList\n",
    "}\n",
    "\n",
    "i=0\n",
    "for speaker1 in speakerSet:\n",
    "    # create node\n",
    "    ulist = []\n",
    "    for k in speakerSet[speaker1].utterSet:\n",
    "        ulist.append(speakerSet[speaker1].utterSet[k])\n",
    "    speaker = {\n",
    "        \"id\": i,\n",
    "        \"name\": speaker1,\n",
    "        \"utterList\": ulist\n",
    "    }\n",
    "    speakerList.append(speaker)\n",
    "    i += 1\n",
    "    \n",
    "    # create link\n",
    "    for speaker2 in speakerSet:\n",
    "        if speaker1 != speaker2:\n",
    "            key=''\n",
    "            if speaker1 <= speaker2:\n",
    "                key = speaker1 + '-' + speaker2\n",
    "            else:\n",
    "                key = speaker2 + '-' + speaker1\n",
    "            \n",
    "            for key1 in speakerSet[speaker1].utterSet:\n",
    "                for key2 in speakerSet[speaker2].utterSet:\n",
    "                    link1 = speakerSet[speaker1].utterSet[key1]\n",
    "                    link2 = speakerSet[speaker2].utterSet[key2]\n",
    "                    if link1['id'] == link2['id']: # if the two speakers have common entity pair 'link1/link2', check whether the speaker pair exists\n",
    "                        if key in convSet: # if the speaker pair already exists, check whether the common entity pair 'link1/link2' exists\n",
    "                            if link1['id'] in convSet[key].utterSet:# if the common entity pair 'link1/link2' also exists, increase frequency\n",
    "                                convSet[key].utterSet[link1['id']]['frequency'] += 1\n",
    "                            else:# if the common entity pair 'link1/link2' doesn't exist, create one, set frequency to 0, because you will visit it again?\n",
    "                                convSet[key].utterSet[link1['id']] = link1\n",
    "                                convSet[key].utterSet[link1['id']]['frequency'] = 1\n",
    "                        else:# if the speaker pair doesn't exist, create one, put current common entity pair into the utterSet\n",
    "                            conv = tempConv(speaker1, speaker2, {})\n",
    "                            conv.utterSet[link1['id']] = link1\n",
    "                            conv.utterSet[link1['id']]['frequency'] = 1\n",
    "                            convSet[key] = conv\n",
    "    \n",
    "for key in convSet:\n",
    "    utterList = []\n",
    "    for key_link in convSet[key].utterSet:\n",
    "        utterList.append(convSet[key].utterSet[key_link])\n",
    "    conv = {\n",
    "        \"source\": convSet[key].source,\n",
    "        \"target\": convSet[key].target,\n",
    "        \"utterList\": utterList\n",
    "    }\n",
    "    convList.append(conv)\n",
    "    \n",
    "#sort\n",
    "for speaker in speakerList:\n",
    "    speaker['utterList'].sort(key=lambda x:x['frequency'], reverse=True)\n",
    "\n",
    "for conv in convList:\n",
    "    conv['utterList'].sort(key=lambda x:x['frequency'], reverse=True)\n",
    "    \n",
    "speakerDict = {\n",
    "    \"nodes\": speakerList,\n",
    "    \"links\": convList\n",
    "}\n",
    "    \n",
    "    \n",
    "#with open('node.json', 'w') as f:\n",
    "#    f.write(json.dumps(outputDict, indent=4, separators=(',', ':')))\n",
    "\n",
    "#with open('highlight.json', 'w') as f:\n",
    "#    f.write(json.dumps(lineList, indent=4, separators=(',', ':')))\n",
    "\n",
    "with open('speakers.json', 'w') as f:\n",
    "    f.write(json.dumps(speakerDict, indent=4, separators=(',', ':')))\n",
    "\n",
    "print(\"Total nodes: \" + str(len(nodeList)))\n",
    "print(\"Total links: \" + str(len(linkList)))\n",
    "print(\"Total speakers: \" + str(len(speakerList)))\n",
    "#print(utterDict['QUESTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aidfs? ', 'df! ', 'dfsdf. ', 'dfsdfs...', ' sdfsdf.\"', 'akekrw']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s = \"aidfs? df! dfsdf. dfsdfs... sdfsdf.\\\"akekrw\"\n",
    "s = re.split(r'(\\. |\\! |\\? |\\.\\.\\.|\\.\\\")',s)\n",
    "s.append(\"\")\n",
    "s = [\"\".join(i) for i in zip(s[0::2],s[1::2])]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trump-trums\n"
     ]
    }
   ],
   "source": [
    "#compare two strings alphabetically\n",
    "token1 = \"trums\"\n",
    "token2 = \"trump\"\n",
    "if token1 <= token2:\n",
    "    print(token1 + '-' + token2)\n",
    "else:\n",
    "    print(token2 + '-' + token1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 4, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5]\n",
    "print(a[::-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
